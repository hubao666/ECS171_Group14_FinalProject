{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target : \n",
      "['Dropout' 'Graduate' 'Enrolled']\n",
      "Dimensions of the dataset :  (4424, 37)\n",
      "\n",
      "Number of samples for each target class:\n",
      "Target\n",
      "Graduate    2209\n",
      "Dropout     1421\n",
      "Enrolled     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", sep=';')\n",
    "\n",
    "print(\"Target : \")\n",
    "print(df['Target'].unique())\n",
    "\n",
    "print(\"Dimensions of the dataset : \", df.shape)\n",
    "\n",
    "print('\\nNumber of samples for each target class:')\n",
    "print(df[\"Target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# check if there is any missing value\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(\"\\nThere are missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Most Significant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi Square \n",
    "we want to find the most significant features, the less the better, while keeping the accuracy 70%+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NN \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that reports the significant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Target')\n",
    "y = df['Target']\n",
    "\n",
    "# normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(data = X_rescaled, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature_chi2(ar_num):\n",
    "    print(f\"Top {ar_num} significant features:\")\n",
    "    selector = SelectKBest(chi2, k=ar_num)\n",
    "    X_selected = selector.fit_transform(X_rescaled, y)\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    selected_features = X.columns[selected_indices]\n",
    "    print(selected_features.tolist())\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Model_less_Feature(ar_selected_features):\n",
    "    df_NN = df.copy()\n",
    "    X = df_NN[ar_selected_features]\n",
    "    y = df_NN['Target']\n",
    "\n",
    "    # normalize data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_rescaled = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(data = X_rescaled, columns = X.columns)\n",
    "\n",
    "    categories = [['Enrolled', 'Graduate', 'Dropout']]\n",
    "    encoder = OneHotEncoder(categories=categories, sparse_output=False)\n",
    "    y = encoder.fit_transform(y.values.reshape(-1,1))\n",
    "\n",
    "    data_train, data_test, class_train, class_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    mlp = MLPClassifier(solver = 'sgd', random_state = 42, activation = 'logistic', learning_rate_init = 0.4, batch_size = 100, hidden_layer_sizes = (23, 17, 12), max_iter = 500)\n",
    "    mlp.fit(data_train, class_train)\n",
    "    pred = mlp.predict(data_test)\n",
    "\n",
    "    print(\"Accuracy : \", accuracy_score(class_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loop to try different sets of features on NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 features selected:\n",
      "Accuracy :  0.7028248587570621\n",
      "14 features selected:\n",
      "Accuracy :  0.6768361581920904\n",
      "12 features selected:\n",
      "Accuracy :  0.7028248587570621\n",
      "10 features selected:\n",
      "Accuracy :  0.7141242937853107\n",
      "8 features selected:\n",
      "Accuracy :  0.7107344632768362\n",
      "6 features selected:\n",
      "Accuracy :  0.6576271186440678\n",
      "4 features selected:\n",
      "Accuracy :  0.6519774011299435\n",
      "2 features selected:\n",
      "Accuracy :  0.2542372881355932\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "for i in range(X.columns.size - 20, 0, -2):\n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_selected = selector.fit_transform(X_rescaled, y)\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    selected_features = X.columns[selected_indices]\n",
    "    print(f'{i} features selected:')\n",
    "    NN_Model_less_Feature(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that 8 features give accuracy 70.97%, which is good. Let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 significant features:\n",
      "['Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)']\n"
     ]
    }
   ],
   "source": [
    "select_feature_chi2(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVM_Model_less_Features(ar_selected_features):\n",
    "    # X = df[ar_selected_features]\n",
    "    # y = df['Target']\n",
    "    columns = ar_selected_features.tolist()\n",
    "    columns.append('Target')\n",
    "    df_svm = df[columns]\n",
    "    df_svm = pd.get_dummies(df_svm, columns=ar_selected_features)\n",
    "    # df_svm = df_svm.astype(int)\n",
    "\n",
    "    svm_train, svm_test = train_test_split(df_svm, test_size=0.2)\n",
    "\n",
    "    X_svm_train, y_svm_train = svm_train.drop(columns=['Target']), svm_train['Target']\n",
    "    X_svm_test, y_svm_test = svm_test.drop(columns=['Target']), svm_test['Target']\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_svm_train)\n",
    "    Z_svm_train = scaler.transform(X_svm_train)\n",
    "    Z_svm_test = scaler.transform(X_svm_test)\n",
    "\n",
    "    svm_li = SVC(kernel='linear')\n",
    "    svm_li.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "    y_pred_li = svm_li.predict(Z_svm_test)\n",
    "    print('Linear Kernel')\n",
    "    print(\"Accuracy : \", accuracy_score(y_svm_test, y_pred_li))\n",
    "\n",
    "\n",
    "    svc_rbf = SVC(kernel='rbf')\n",
    "    svc_rbf.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "    y_pred_rbf = svc_rbf.predict(Z_svm_test)\n",
    "    print('RBF Kernel')\n",
    "    print(\"Accuracy : \", accuracy_score(y_svm_test, y_pred_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this chunk of code takes ages (4mins, and the accuracy varies randomly, but the result that 4 features are sufficient to predict is consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.672316384180791\n",
      "RBF Kernel\n",
      "Accuracy :  0.7265536723163842\n",
      "\n",
      "9 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.7163841807909604\n",
      "RBF Kernel\n",
      "Accuracy :  0.7378531073446327\n",
      "\n",
      "8 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.7209039548022599\n",
      "RBF Kernel\n",
      "Accuracy :  0.7401129943502824\n",
      "\n",
      "7 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.711864406779661\n",
      "RBF Kernel\n",
      "Accuracy :  0.7231638418079096\n",
      "\n",
      "6 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.7016949152542373\n",
      "RBF Kernel\n",
      "Accuracy :  0.7050847457627119\n",
      "\n",
      "5 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.6836158192090396\n",
      "RBF Kernel\n",
      "Accuracy :  0.6768361581920904\n",
      "\n",
      "4 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.7231638418079096\n",
      "RBF Kernel\n",
      "Accuracy :  0.7084745762711865\n",
      "\n",
      "3 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.6779661016949152\n",
      "RBF Kernel\n",
      "Accuracy :  0.6734463276836158\n",
      "\n",
      "2 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.5163841807909605\n",
      "RBF Kernel\n",
      "Accuracy :  0.5209039548022599\n",
      "\n",
      "1 features selected:\n",
      "Linear Kernel\n",
      "Accuracy :  0.49830508474576274\n",
      "RBF Kernel\n",
      "Accuracy :  0.49830508474576274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.columns.size -26, 0, -1): # since SVM takes more time to train, we start with even less features than NN Model \n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_selected = selector.fit_transform(X_rescaled, y)\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    selected_features = X.columns[selected_indices]\n",
    "    print(f'{i} features selected:')\n",
    "    SVM_Model_less_Features(selected_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with __Linear__ kernel gives 72.32% accuracy with only `4` features. Lets see what are them.\n",
    "\n",
    "we can also observe a significant drop in terms of accuracy from 4 features to 3 features (70% ish to <68%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 significant features:\n",
      "['Debtor', 'Scholarship holder', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)']\n"
     ]
    }
   ],
   "source": [
    "select_feature_chi2(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Deci_Tree_less_Features(ar_selected_features):\n",
    "    columns = ar_selected_features.tolist()\n",
    "    columns = columns + ['Target']\n",
    "    df_DTC = df[columns]\n",
    "    \n",
    "    train, test = train_test_split(df_DTC, test_size=0.2)\n",
    "\n",
    "    X_train, y_train = train.drop(columns=['Target']), train['Target']\n",
    "    X_test, y_test = test.drop(columns=['Target']), test['Target']\n",
    "\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=1234)\n",
    "    dtree_model = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 features selected:\n",
      "Accuracy :  0.6734463276836158\n",
      "\n",
      "14 features selected:\n",
      "Accuracy :  0.6485875706214689\n",
      "\n",
      "12 features selected:\n",
      "Accuracy :  0.6361581920903955\n",
      "\n",
      "10 features selected:\n",
      "Accuracy :  0.63954802259887\n",
      "\n",
      "8 features selected:\n",
      "Accuracy :  0.6576271186440678\n",
      "\n",
      "6 features selected:\n",
      "Accuracy :  0.6384180790960452\n",
      "\n",
      "4 features selected:\n",
      "Accuracy :  0.6790960451977401\n",
      "\n",
      "2 features selected:\n",
      "Accuracy :  0.5559322033898305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.columns.size - 20, 0, -2): # since SVM takes more time to train, we start with even less features than NN Model \n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_selected = selector.fit_transform(X_rescaled, y)\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    selected_features = X.columns[selected_indices]\n",
    "    print(f'{i} features selected:')\n",
    "    Deci_Tree_less_Features(selected_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifer doesn't have a good performance on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def Random_forest_less_Features(ar_selected_features):\n",
    "    columns = ar_selected_features.tolist()\n",
    "    columns = columns + ['Target']\n",
    "    df_RDC = df[columns]\n",
    "    \n",
    "    train, test = train_test_split(df_RDC, test_size=0.2)\n",
    "\n",
    "    X_train, y_train = train.drop(columns=['Target']), train['Target']\n",
    "    X_test, y_test = test.drop(columns=['Target']), test['Target']\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 features selected:\n",
      "Accuracy :  0.727683615819209\n",
      "\n",
      "10 features selected:\n",
      "Accuracy :  0.7333333333333333\n",
      "\n",
      "9 features selected:\n",
      "Accuracy :  0.7468926553672316\n",
      "\n",
      "8 features selected:\n",
      "Accuracy :  0.7254237288135593\n",
      "\n",
      "7 features selected:\n",
      "Accuracy :  0.7016949152542373\n",
      "\n",
      "6 features selected:\n",
      "Accuracy :  0.6858757062146893\n",
      "\n",
      "5 features selected:\n",
      "Accuracy :  0.6745762711864407\n",
      "\n",
      "4 features selected:\n",
      "Accuracy :  0.6813559322033899\n",
      "\n",
      "3 features selected:\n",
      "Accuracy :  0.6440677966101694\n",
      "\n",
      "2 features selected:\n",
      "Accuracy :  0.5559322033898305\n",
      "\n",
      "1 features selected:\n",
      "Accuracy :  0.519774011299435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.columns.size - 25, 0, -1): \n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_selected = selector.fit_transform(X_rescaled, y)\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Get the names of the selected features\n",
    "    selected_features = X.columns[selected_indices]\n",
    "    print(f'{i} features selected:')\n",
    "    Random_forest_less_Features(selected_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Features give 70.17% accuracy, but SVM is better(only 4 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with `linear` Model using the least number of features (4) and maintains the accuracy 70%+. So lets explore more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def report_confusion_matrices_and_other_measurement(ar_class_test, ar_pred):\n",
    "    print(\"Accuracy : \", accuracy_score(ar_class_test, ar_pred))\n",
    "    mcm =  multilabel_confusion_matrix(ar_class_test, ar_pred)\n",
    "    categories = [['Enrolled', 'Graduate', 'Dropout']]\n",
    "    unique_labels = categories[0]\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        print(f\"Confusion Matrix for label '{label}':\")\n",
    "        cm = mcm[i]\n",
    "        print(cm)\n",
    "        tp = cm[1, 1]\n",
    "        fn = cm[1, 0]\n",
    "        fp = cm[0, 1]\n",
    "        tn = cm[0, 0]\n",
    "        \n",
    "        print(f\"True Positive (TP)  : {tp}\")\n",
    "        print(f\"False Negative (FN) : {fn}\")\n",
    "        print(f\"False Positive (FP) : {fp}\")\n",
    "        print(f\"True Negative (TN)  : {tn}\")\n",
    "        print()\n",
    "    \n",
    "   # print(\"Mean Square Error : \", mean_squared_error(ar_class_test, ar_pred))\n",
    "    print(\"Classification Report : \")\n",
    "    print(classification_report(ar_class_test, ar_pred))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVM_Model_less_Features_with_report(ar_selected_features):\n",
    "    # X = df[ar_selected_features]\n",
    "    # y = df['Target']\n",
    "    columns = ar_selected_features.tolist()\n",
    "    columns.append('Target')\n",
    "    df_svm = df[columns]\n",
    "    df_svm = pd.get_dummies(df_svm, columns=ar_selected_features)\n",
    "    # df_svm = df_svm.astype(int)\n",
    "\n",
    "    svm_train, svm_test = train_test_split(df_svm, test_size=0.2)\n",
    "\n",
    "    X_svm_train, y_svm_train = svm_train.drop(columns=['Target']), svm_train['Target']\n",
    "    X_svm_test, y_svm_test = svm_test.drop(columns=['Target']), svm_test['Target']\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_svm_train)\n",
    "    Z_svm_train = scaler.transform(X_svm_train)\n",
    "    Z_svm_test = scaler.transform(X_svm_test)\n",
    "\n",
    "    svm_li = SVC(kernel='linear')\n",
    "    svm_li.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "    y_pred_li = svm_li.predict(Z_svm_test)\n",
    "    print('Linear Kernel')\n",
    "    report_confusion_matrices_and_other_measurement(y_svm_test, y_pred_li)\n",
    "\n",
    "    svc_rbf = SVC(kernel='rbf')\n",
    "    svc_rbf.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "    y_pred_rbf = svc_rbf.predict(Z_svm_test)\n",
    "    print('RBF Kernel')\n",
    "    report_confusion_matrices_and_other_measurement(y_svm_test, y_pred_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 significant features:\n",
      "['Debtor', 'Scholarship holder', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)']\n",
      "Linear Kernel\n",
      "Accuracy :  0.7129943502824859\n",
      "Confusion Matrix for label 'Enrolled':\n",
      "[[554  54]\n",
      " [ 79 198]]\n",
      "True Positive (TP)  : 198\n",
      "False Negative (FN) : 79\n",
      "False Positive (FP) : 54\n",
      "True Negative (TN)  : 554\n",
      "\n",
      "Confusion Matrix for label 'Graduate':\n",
      "[[631  83]\n",
      " [111  60]]\n",
      "True Positive (TP)  : 60\n",
      "False Negative (FN) : 111\n",
      "False Positive (FP) : 83\n",
      "True Negative (TN)  : 631\n",
      "\n",
      "Confusion Matrix for label 'Dropout':\n",
      "[[331 117]\n",
      " [ 64 373]]\n",
      "True Positive (TP)  : 373\n",
      "False Negative (FN) : 64\n",
      "False Positive (FP) : 117\n",
      "True Negative (TN)  : 331\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.79      0.71      0.75       277\n",
      "    Enrolled       0.42      0.35      0.38       171\n",
      "    Graduate       0.76      0.85      0.80       437\n",
      "\n",
      "    accuracy                           0.71       885\n",
      "   macro avg       0.66      0.64      0.65       885\n",
      "weighted avg       0.70      0.71      0.71       885\n",
      "\n",
      "RBF Kernel\n",
      "Accuracy :  0.6949152542372882\n",
      "Confusion Matrix for label 'Enrolled':\n",
      "[[546  62]\n",
      " [ 74 203]]\n",
      "True Positive (TP)  : 203\n",
      "False Negative (FN) : 74\n",
      "False Positive (FP) : 62\n",
      "True Negative (TN)  : 546\n",
      "\n",
      "Confusion Matrix for label 'Graduate':\n",
      "[[677  37]\n",
      " [153  18]]\n",
      "True Positive (TP)  : 18\n",
      "False Negative (FN) : 153\n",
      "False Positive (FP) : 37\n",
      "True Negative (TN)  : 677\n",
      "\n",
      "Confusion Matrix for label 'Dropout':\n",
      "[[277 171]\n",
      " [ 43 394]]\n",
      "True Positive (TP)  : 394\n",
      "False Negative (FN) : 43\n",
      "False Positive (FP) : 171\n",
      "True Negative (TN)  : 277\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.77      0.73      0.75       277\n",
      "    Enrolled       0.33      0.11      0.16       171\n",
      "    Graduate       0.70      0.90      0.79       437\n",
      "\n",
      "    accuracy                           0.69       885\n",
      "   macro avg       0.60      0.58      0.56       885\n",
      "weighted avg       0.65      0.69      0.65       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features = select_feature_chi2(4)\n",
    "SVM_Model_less_Features_with_report(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need some interpretations..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
